{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ETL PROCESS\n",
    "\n",
    "Database (Snowflake DW)\n",
    "This Ingest process uses the Snowflake data warehouse to hold the external data.\n",
    "“Snowflake is an analytics data warehouse delivered as software as a service (SaaS). Snowflake’s data warehouse does not rely on an existing database or “big data” software platform like Hadoop. Snowflake’s data warehouse uses a new SQL database engine with a unique architecture designed for the cloud.”\n",
    "For more information: https://www.snowflake.com/workloads/data-warehouse-modernization/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definition of functions\n",
    "The necessary functions for the ETL process will be defined below.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warehouse Functions\n",
    "Next we will define the functions that will allow us to interact between Python and our Warehouse\n",
    "Connection for Snowflake DW\n",
    "Note: Create venv file with variable user and password, this for security of our data.\n",
    "This is a Personal warehouse, but the database and schemas have been isolated to work in this specific case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "\"\"\"\n",
    "Set up connection\n",
    "\"\"\"\n",
    "load_dotenv()\n",
    "con = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "    database='DEEL_ANALYTICS',\n",
    "    schema='SRC_RAW_GLOBEPAY',\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def put_data_in_snowflake( df, schema: str, table_name: str, ):\n",
    "    df_1 = df.copy()\n",
    "    return write_pandas(con, df_1, table_name=table_name.upper(), schema=schema.upper(), auto_create_table=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions to handling file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def csv_to_df(filename):\n",
    "    \"\"\"\n",
    "    Function for read csv File\n",
    "    :param filename:\n",
    "    :return: Dataframe parsing from CSV\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=',', encoding='UTF8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Function for standardize columns of a dataframe\n",
    "    :param df:\n",
    "    :return: Columns standardized\n",
    "    \"\"\"\n",
    "\n",
    "    df_1 = df.copy()\n",
    "    new_columns = []\n",
    "    count = 1\n",
    "    for x in df_1.columns:\n",
    "        x.lower()\n",
    "        mt = x.lower().maketrans(\"/'.,:¿?/()ÁÉÍÓÚáéíóú\", '          aeiouaeiou')\n",
    "        new_column = x.translate(mt).strip().replace(\" \", \"_\").upper()\n",
    "        new_columns.append(new_column)\n",
    "\n",
    "    return new_columns\n",
    "\n",
    "\n",
    "def get_columns_duplicated(df):\n",
    "    \"\"\"\n",
    "    Function to get a list of columns duplicated in a dataframe\n",
    "    :param df:\n",
    "    :return: List of columns duplicated in a dataframe\n",
    "    \"\"\"\n",
    "    duplicated_columns = []\n",
    "    for c in df.columns:\n",
    "\n",
    "        if df.columns.to_list().count(c) > 1:\n",
    "            duplicated_columns.append(c)\n",
    "\n",
    "    return list(set(duplicated_columns))\n",
    "\n",
    "\n",
    "def rename_duplicated_columns(df):\n",
    "    \"\"\"\n",
    "    Function for rename duplicated values in a dataframe\n",
    "    :param df: Input dataframe\n",
    "    :return: Dataframe with renamed columns\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    count = 1\n",
    "    duplicated_columns = get_columns_duplicated(df)\n",
    "\n",
    "    for duplicated_column in duplicated_columns:\n",
    "        for column in df.columns:\n",
    "            if column == duplicated_column:\n",
    "                cols.append(f'{duplicated_column}_{count}')\n",
    "                count += 1\n",
    "                continue\n",
    "            cols.append(column)\n",
    "        df.columns = cols\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## running pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Globepay Chargeback Report - Globepay Chargeback Report.csv',\n 'Globepay Acceptance Report - Globepay Acceptance Report.csv']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir('sources')\n",
    "file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything was ok for table Globepay Chargeback Report - Globepay Chargeback Report  !\n",
      "Everything was ok for table Globepay Acceptance Report - Globepay Acceptance Report  !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in file_names:\n",
    "\n",
    "    #1.  Read local File\n",
    "    file_name = f\"sources/{file}\"\n",
    "    df_raw = csv_to_df(file_name)\n",
    "\n",
    "    #2. Handling column names\n",
    "    df_raw.columns = standardize_columns(df_raw)\n",
    "\n",
    "    #3. Check and handling duplicated columns\n",
    "    rename_duplicated_columns(df_raw)\n",
    "    table_name = file.replace('.csv','')\n",
    "\n",
    "    #4. Send raw data to Snowflake :3\n",
    "    put_data_in_snowflake(df_raw, 'SRC_RAW_GLOBEPAY',table_name )\n",
    "    print(\"Everything was ok for table\", table_name, \" !\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
